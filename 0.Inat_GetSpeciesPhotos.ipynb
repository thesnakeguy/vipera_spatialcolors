{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-GhouIbyjl_"
   },
   "source": [
    "# import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHnqp4i1l4Oz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import shutil\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from pyinaturalist import *\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from typing_extensions import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: get place code for limiting search spatially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m{\u001b[0m\n",
      "    \u001b[1;36m6753\u001b[0m: \u001b[32m'France'\u001b[0m,\n",
      "    \u001b[1;36m10577\u001b[0m: \u001b[32m'ÃŽle-de-France'\u001b[0m,\n",
      "    \u001b[1;36m11367\u001b[0m: \u001b[32m'Fort-de-France'\u001b[0m,\n",
      "    \u001b[1;36m104968\u001b[0m: \u001b[32m'Francesti'\u001b[0m,\n",
      "    \u001b[1;36m30178\u001b[0m: \u001b[32m'Seine-Saint-Denis'\u001b[0m,\n",
      "    \u001b[1;36m38738\u001b[0m: \u001b[32m'Fort-de-France'\u001b[0m,\n",
      "    \u001b[1;36m99548\u001b[0m: \u001b[32m'Hauts-de-Seine'\u001b[0m,\n",
      "    \u001b[1;36m99550\u001b[0m: \u001b[32m\"Val-d'Oise\"\u001b[0m,\n",
      "    \u001b[1;36m99546\u001b[0m: \u001b[32m'Val-de-Marne'\u001b[0m,\n",
      "    \u001b[1;36m30182\u001b[0m: \u001b[32m'Yvelines'\u001b[0m\n",
      "\u001b[1m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = get_places_autocomplete(q='France')\n",
    "pprint({p['id']: p['name'] for p in  response['results']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdGOwyfxvvHX"
   },
   "source": [
    "# Initialize function to scrap images from Inaturalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 193,
     "status": "ok",
     "timestamp": 1737719532758,
     "user": {
      "displayName": "Pablo Deschepper",
      "userId": "11062026878236535060"
     },
     "user_tz": -60
    },
    "id": "2eI96jmOl92k"
   },
   "outputs": [],
   "source": [
    "# Scrape Images and Metadata\n",
    "\n",
    "# Here we make a function to save photos of a target species locally and save necessary metada: latitude, longitude, observation ID, photo IDs, Photo urls. Metadata is saved as a dictionary and photos are saved in a directory specified by the user.\n",
    "# Metadata example:\n",
    "# observation_id   latitude  longitude    photo_ids  \\\n",
    "# 0       259323505  57.736160  10.629406  [465488543]\n",
    "# 1       259193935  48.037273  11.509971  [465220887]\n",
    "# 2       258982331  49.385485  19.790977  [464790765]\n",
    "# 3       258835093  46.517517   9.908752  [464493974]\n",
    "# 4       258811645  52.674268   6.516881  [464445739]\n",
    "#\n",
    "#                                               photos\n",
    "# 0  [https://inaturalist-open-data.s3.amazonaws.co...\n",
    "# 1  [https://inaturalist-open-data.s3.amazonaws.co...\n",
    "# 2  [https://static.inaturalist.org/photos/4647907...\n",
    "# 3  [https://static.inaturalist.org/photos/4644939...\n",
    "# 4  [https://static.inaturalist.org/photos/4644457...\n",
    "\n",
    "# Parameters\n",
    "output_dir = \"/mnt/c/Users/pdeschepper/Desktop/PERSONAL/DeepLearning/ImageSegmentation/Snakes_ImageSegmentation_keras/Vipera_segmentation_test_dataset\"\n",
    "max_accuracy = 1000\n",
    "record_limiter = 5000\n",
    "\n",
    "def scrape_inaturalist_images(species_name):\n",
    "    \"\"\"Scrape images and metadata for a target species from iNaturalist.\"\"\"\n",
    "    observations = []\n",
    "    page = 1\n",
    "    per_page = 30\n",
    "\n",
    "    while len(observations) < record_limiter:\n",
    "        print(f\"Fetching page {page}...\") # Added for debugging/tracking\n",
    "        try:\n",
    "            response = get_observations(\n",
    "                taxon_id=species_name,\n",
    "                photos=True,\n",
    "                geo=True,\n",
    "                place_id=6753,\n",
    "                identified=True,\n",
    "                geoprivacy='open',\n",
    "                acc_below=max_accuracy,\n",
    "                page=page,\n",
    "                per_page=per_page\n",
    "            )\n",
    "        \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            # Check specifically for the 429 error\n",
    "            if '429' in str(e):\n",
    "                print(\"ðŸš¨ Rate limit hit (429)! Pausing for 10 seconds...\")\n",
    "                time.sleep(10) # Wait a minute if severely throttled\n",
    "                continue # Retry the same page\n",
    "            else:\n",
    "                # Handle other HTTP errors\n",
    "                print(f\"An unexpected HTTP error occurred: {e}\")\n",
    "                break\n",
    "\n",
    "        # Check for empty results on a page, which might indicate the end\n",
    "        if not response.get('results'):\n",
    "            break\n",
    "\n",
    "        # Add observations from current page\n",
    "        observations.extend(response.get('results', []))\n",
    "        \n",
    "        print(f\"  -> Collected {len(observations)} of {record_limiter} so far.\")\n",
    "\n",
    "        # Check for exit conditions\n",
    "        if response.get('page') == response.get('pages') or len(observations) >= record_limiter:\n",
    "            break\n",
    "\n",
    "        # Increment page for next iteration\n",
    "        page += 1\n",
    "        \n",
    "        # Add a delay between API calls\n",
    "        time.sleep(2) # Pause for 2 seconds to respect API limits\n",
    "\n",
    "    # Limit records to record_limiter if exceeded\n",
    "    observations = observations[:record_limiter]\n",
    "\n",
    "    # Ensure the output directory exists and clear if not empty\n",
    "    if os.path.exists(output_dir) and os.listdir(output_dir):\n",
    "        # Only remove files if they match the expected pattern to be safe\n",
    "        for file in os.listdir(output_dir):\n",
    "            if file.endswith((\".jpg\", \".csv\")):\n",
    "                 os.remove(os.path.join(output_dir, file))\n",
    "    else:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Process observations and store metadata\n",
    "    metadata = []\n",
    "    # Use requests.Session for efficient downloading\n",
    "    s = requests.Session() \n",
    "    \n",
    "    for obs in observations:\n",
    "        observation_id = obs.get('id', None)\n",
    "        latitude = obs.get('geojson', {}).get('coordinates', [None, None])[1]\n",
    "        longitude = obs.get('geojson', {}).get('coordinates', [None, None])[0]\n",
    "        photos = obs.get('photos', [])\n",
    "\n",
    "        # Collect high-resolution photo URLs and IDs\n",
    "        photo_urls = [photo.get('url', \"\").replace(\"square\", \"original\") for photo in photos]\n",
    "        photo_ids = [photo.get('id', None) for photo in photos]\n",
    "\n",
    "        # Download and save photos\n",
    "        for i, img_url in enumerate(photo_urls):\n",
    "            photo_id = photo_ids[i]\n",
    "            img_path = os.path.join(output_dir, f\"{photo_id}.jpg\")\n",
    "            \n",
    "            # Skip if URL is empty or None\n",
    "            if not img_url:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Use the session for downloading\n",
    "                img_response = s.get(img_url, timeout=10)\n",
    "                img_response.raise_for_status() # Check for bad status codes (like 404)\n",
    "                \n",
    "                with open(img_path, 'wb') as f:\n",
    "                    f.write(img_response.content)\n",
    "                \n",
    "                # Add a smaller delay between photo downloads if needed, but usually not required\n",
    "                # time.sleep(0.5) \n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading photo {photo_id} from {img_url}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Append metadata for the observation\n",
    "        metadata.append({\n",
    "            \"observation_id\": observation_id,\n",
    "            \"latitude\": latitude,\n",
    "            \"longitude\": longitude,\n",
    "            \"photo_ids\": photo_ids,\n",
    "            \"photos\": photo_urls\n",
    "        })\n",
    "        \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlHZPoKpzO8N"
   },
   "source": [
    "# Use scraping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "executionInfo": {
     "elapsed": 733963,
     "status": "ok",
     "timestamp": 1737720271150,
     "user": {
      "displayName": "Pablo Deschepper",
      "userId": "11062026878236535060"
     },
     "user_tz": -60
    },
    "id": "c4IVi31wuq1F",
    "outputId": "651deb2e-b7b0-4b64-b141-830243ced4ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1...\n",
      "  -> Collected 30 of 5000 so far.\n",
      "Fetching page 2...\n",
      "  -> Collected 60 of 5000 so far.\n",
      "Fetching page 3...\n",
      "  -> Collected 90 of 5000 so far.\n",
      "Fetching page 4...\n",
      "  -> Collected 120 of 5000 so far.\n",
      "Fetching page 5...\n",
      "  -> Collected 150 of 5000 so far.\n",
      "Fetching page 6...\n",
      "  -> Collected 180 of 5000 so far.\n",
      "Fetching page 7...\n",
      "  -> Collected 210 of 5000 so far.\n",
      "Fetching page 8...\n",
      "  -> Collected 240 of 5000 so far.\n",
      "Fetching page 9...\n",
      "  -> Collected 270 of 5000 so far.\n",
      "Fetching page 10...\n",
      "  -> Collected 300 of 5000 so far.\n",
      "Fetching page 11...\n",
      "  -> Collected 330 of 5000 so far.\n",
      "Fetching page 12...\n",
      "  -> Collected 360 of 5000 so far.\n",
      "Fetching page 13...\n",
      "  -> Collected 390 of 5000 so far.\n",
      "Fetching page 14...\n",
      "  -> Collected 420 of 5000 so far.\n",
      "Fetching page 15...\n",
      "  -> Collected 450 of 5000 so far.\n",
      "Fetching page 16...\n",
      "  -> Collected 480 of 5000 so far.\n",
      "Fetching page 17...\n",
      "  -> Collected 510 of 5000 so far.\n",
      "Fetching page 18...\n",
      "  -> Collected 540 of 5000 so far.\n",
      "Fetching page 19...\n",
      "  -> Collected 570 of 5000 so far.\n",
      "Fetching page 20...\n",
      "  -> Collected 600 of 5000 so far.\n",
      "Fetching page 21...\n",
      "  -> Collected 630 of 5000 so far.\n",
      "Fetching page 22...\n",
      "  -> Collected 660 of 5000 so far.\n",
      "Fetching page 23...\n",
      "  -> Collected 690 of 5000 so far.\n",
      "Fetching page 24...\n",
      "  -> Collected 720 of 5000 so far.\n",
      "Fetching page 25...\n",
      "  -> Collected 750 of 5000 so far.\n",
      "Fetching page 26...\n",
      "  -> Collected 780 of 5000 so far.\n",
      "Fetching page 27...\n",
      "  -> Collected 810 of 5000 so far.\n",
      "Fetching page 28...\n",
      "  -> Collected 840 of 5000 so far.\n",
      "Fetching page 29...\n",
      "  -> Collected 870 of 5000 so far.\n",
      "Fetching page 30...\n",
      "  -> Collected 900 of 5000 so far.\n",
      "Fetching page 31...\n",
      "  -> Collected 930 of 5000 so far.\n",
      "Fetching page 32...\n",
      "  -> Collected 960 of 5000 so far.\n",
      "Fetching page 33...\n",
      "  -> Collected 990 of 5000 so far.\n",
      "Fetching page 34...\n",
      "  -> Collected 1020 of 5000 so far.\n",
      "Fetching page 35...\n",
      "  -> Collected 1050 of 5000 so far.\n",
      "Fetching page 36...\n",
      "  -> Collected 1080 of 5000 so far.\n",
      "Fetching page 37...\n",
      "  -> Collected 1110 of 5000 so far.\n",
      "Fetching page 38...\n",
      "  -> Collected 1140 of 5000 so far.\n",
      "Fetching page 39...\n",
      "  -> Collected 1170 of 5000 so far.\n",
      "Fetching page 40...\n",
      "  -> Collected 1200 of 5000 so far.\n",
      "Fetching page 41...\n",
      "  -> Collected 1230 of 5000 so far.\n",
      "Fetching page 42...\n",
      "  -> Collected 1260 of 5000 so far.\n",
      "Fetching page 43...\n",
      "  -> Collected 1290 of 5000 so far.\n",
      "Fetching page 44...\n",
      "  -> Collected 1292 of 5000 so far.\n",
      "Fetching page 45...\n",
      "Error downloading photo 571360885 from https://inaturalist-open-data.s3.amazonaws.com/photos/571360885/original.jpg: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Error downloading photo 195573036 from https://inaturalist-open-data.s3.amazonaws.com/photos/195573036/original.jpeg: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "   observation_id   latitude  longitude  \\\n",
      "0       320891142  43.960629   6.270843   \n",
      "1       320218823  45.406588   5.910679   \n",
      "2       320218774  45.406601   5.910893   \n",
      "3       320149996  42.504756   2.095933   \n",
      "4       320027698  44.158968   7.250449   \n",
      "\n",
      "                                      photo_ids  \\\n",
      "0                                   [580253137]   \n",
      "1             [578933598, 578933659, 578933737]   \n",
      "2                                   [578933507]   \n",
      "3                                   [578801334]   \n",
      "4  [578561021, 578561050, 578561072, 578561102]   \n",
      "\n",
      "                                              photos  \n",
      "0  [https://inaturalist-open-data.s3.amazonaws.co...  \n",
      "1  [https://inaturalist-open-data.s3.amazonaws.co...  \n",
      "2  [https://inaturalist-open-data.s3.amazonaws.co...  \n",
      "3  [https://static.inaturalist.org/photos/5788013...  \n",
      "4  [https://inaturalist-open-data.s3.amazonaws.co...  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;36m1292\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usage\n",
    "species_name = \"30929\" # Vipera aspis\n",
    "metadata = scrape_inaturalist_images(species_name)\n",
    "\n",
    "# Convert metadata to a DataFrame for easier visualization\n",
    "df = pd.DataFrame(metadata)\n",
    "\n",
    "# Save metadata to a CSV file\n",
    "metadata_path = os.path.join(output_dir, \"metadata.csv\")\n",
    "df.to_csv(metadata_path, index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())\n",
    "len(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wscP1P_bwxvR"
   },
   "source": [
    "# Open a photo and check size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 898,
     "output_embedded_package_id": "1VwqEyVL0ryO-ftXk1C0HCEl6zK19TN7T"
    },
    "executionInfo": {
     "elapsed": 3650,
     "status": "ok",
     "timestamp": 1737722307898,
     "user": {
      "displayName": "Pablo Deschepper",
      "userId": "11062026878236535060"
     },
     "user_tz": -60
    },
    "id": "JMIZ5flyyUPK",
    "outputId": "f11aa0be-599d-4ead-a3aa-72b214833ff4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a list of all image files in the directory\n",
    "image_files = []\n",
    "for f in os.listdir(output_dir):\n",
    "    if os.path.isfile(os.path.join(output_dir, f)):\n",
    "        image_files.append(f)\n",
    "\n",
    "# Select a random image file\n",
    "random_image_file = random.choice(image_files)\n",
    "\n",
    "# Construct the full path to the random image\n",
    "random_image_path = os.path.join(output_dir, random_image_file)\n",
    "\n",
    "# Open the random image\n",
    "image = Image.open(random_image_path)\n",
    "\n",
    "# Get dimensions\n",
    "width, height = image.size\n",
    "print(f\"Width: {width}, Height: {height}\")\n",
    "\n",
    "# Display the image (optional)\n",
    "display(image)\n",
    "\n",
    "print(image_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTK6Vm-rVy8V"
   },
   "source": [
    "# Zip photos (.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 182,
     "status": "ok",
     "timestamp": 1737723424933,
     "user": {
      "displayName": "Pablo Deschepper",
      "userId": "11062026878236535060"
     },
     "user_tz": -60
    },
    "id": "8Ct4kplrU4Uw",
    "outputId": "d77ef116-9ed6-4325-ea52-b6a77fb90ad7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object _walk at 0x79304ec91a20>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def zip_jpg_files(output_directory, zip_file_name=\"images.zip\"):\n",
    "    \"\"\"\n",
    "    This function zips all the .jpg files in a given directory.\n",
    "\n",
    "    Args:\n",
    "        output_directory: The directory where the .jpg files are located.\n",
    "        zip_file_name: The name of the zip file to be created (default: \"images.zip\").\n",
    "\n",
    "    Returns:\n",
    "        None. It creates a zip file in the output directory.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the full path for the zip file\n",
    "    zip_file_path = os.path.join(output_directory, zip_file_name)\n",
    "\n",
    "    # Open the zip file in write mode ('w')\n",
    "    with zipfile.ZipFile(zip_file_path, 'w') as zip_file:\n",
    "        # Go through all the files and folders in the output directory\n",
    "        for current_folder, subfolders, files in os.walk(output_directory):\n",
    "            # Check each file in the current folder\n",
    "            for file_name in files:\n",
    "                # If the file ends with '.jpg', add it to the zip file\n",
    "                if file_name.endswith('.jpg'):\n",
    "                    # Get the full path of the file\n",
    "                    file_path = os.path.join(current_folder, file_name)\n",
    "\n",
    "                    # Add the file to the zip file using its original name\n",
    "                    zip_file.write(file_path, arcname=file_name)\n",
    "\n",
    "    # Print a message to confirm the zip file creation\n",
    "    print(f\"All .jpg files in '{output_directory}' have been zipped to '{zip_file_name}'\")\n",
    "\n",
    "\n",
    "# Call the zip function\n",
    "zip_jpg_files(output_dir)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPUwx0SYS+P94ka9SboodpV",
   "mount_file_id": "1tQu3h38qBczfk6K_PVtK0MwEtZix7Ic3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
